// Literature Review Data - structured to address the research questions
const literatureReviews = [
  {
    id: 1,
    title: "RQ1: Instructors' and Students' Perceptions of Readiness to Use GPT",
    category: "Readiness",
    summary: "An exploration of how instructors and students perceive their readiness to use GPT in teaching and learning contexts.",
    content: "Faculty and students demonstrate varying levels of readiness to adopt GPT in educational settings. According to Sanders & Mukhari (2024), successful AI integration in blended learning requires robust management support, enhanced training opportunities, reliable technological infrastructure, and sufficient time allocation for lecturers. The need for institutional support emerged as a consistent theme across multiple studies.\n\nMarquardson (2024) found that AI tools enhance self-directed learning when students are well-trained in their use, suggesting that readiness is not inherent but can be developed through proper orientation. Similarly, Husain (2024) concluded that while ChatGPT aids programming instruction, its effective implementation requires careful application and guidance.\n\nA study by Cabero-Almenara et al. (2024) revealed high AI acceptance among teachers, though this was significantly influenced by demographic factors and teaching modalities. This variability in readiness was further supported by Hazaimeh & Al-Ansi (2024), who identified that positive attitudes, digital skills, and openness drive AI acceptance among both teaching staff and students.\n\nPerceptions of readiness often differ between generations. Chan & Lee (2023) observed that Gen Z students typically show more enthusiasm for GenAI compared to Gen X and Millennial teachers, who emphasize ethical and pedagogical concerns. This generational gap in readiness perceptions highlights the need for differentiated support approaches.\n\nImportantly, Wang et al. (2023) conceptualized teachers' AI readiness as distinct from TPACK (Technological Pedagogical Content Knowledge), measuring it through awareness, knowledge, and ability. Their research emphasized that AI readiness is crucial for effective AI integration in teaching and often requires specific preparation beyond general technological competence.\n\nCross-cultural differences also influence readiness perceptions. Ma et al. (2024) found significant disparities in AI usage prevalence, attitudes, and behavioral intentions between Chinese and international university students, with cultural backgrounds and prior technological exposure significantly influencing students' attitudes toward AI adoption.",
    reflectionPrompts: [
      "How might institutions better prepare faculty and students for effective GPT adoption?",
      "What factors most influence instructors' and students' perceived readiness to use GPT?",
      "How can differences in readiness across disciplines be effectively addressed?"
    ],
    hasWordCloud: true
  },
  {
    id: 2,
    title: "RQ2: Ethical Concerns in Humanities, Social Sciences, and STEM",
    category: "Ethics",
    summary: "Examining the ethical concerns instructors and students have regarding GPT adoption across different disciplinary contexts.",
    content: "Research reveals diverse ethical concerns regarding GPT and AI adoption across academic disciplines. Perkins (2023) highlights how advanced language models risk undermining academic integrity through undetectable AI-generated plagiarism. Similarly, Dehouche (2021) examines how GPT-3 can facilitate academic misconduct by producing human-like text that evades traditional detection methods, requiring updated plagiarism detection approaches.\n\nCrawford et al. (2023) emphasize that leadership, character development, and authentic assessment are essential components for the ethical use of AI in learning. This aligns with Yeo's (2022) findings that educators must proactively incorporate AI tools into curricula to maintain academic integrity standards, as AI writing tools fundamentally challenge traditional concepts of authorship and originality.\n\nKooli (2023) identifies broader ethical implications, including data privacy concerns, potential biases, and risks of over-reliance on automated systems in educational settings. The study argues for establishing robust ethical frameworks for responsible AI deployment in academia. Chan (2023) proposes a comprehensive AI Ecological Education Policy Framework encompassing Pedagogical, Governance, and Operational dimensions to guide ethical AI integration.\n\nYan et al. (2023) conducted a systematic review highlighting challenges including low technological readiness, lack of transparency, and privacy concerns across 53 educational applications of LLMs. These findings support Walter's (2024) argument for AI literacy and prompt engineering as crucial components for ethically integrating AI into education.\n\nStakeholder perspectives on AI ethics in higher education reveal significant differences across disciplines. In computer science, Wang et al. (2023) found instructors express concerns about dependency, ethics, and proper implementation of AI tools. Holmes et al. (2023) emphasize the need for clear ethical guidelines addressing privacy, bias, and accountability concerns in distance learning contexts.\n\nKlayklung et al. (2023) and Hazari (2024) advocate for AI literacy courses in higher education curricula to foster critical analysis, responsible use of AI, and awareness of ethical concerns and biases. In a study on student behavior, Nguyen et al. (2024) identified ethical problems including plagiarism and information security risks when students overrely on ChatGPT, potentially hindering independent thinking and critical evaluation skills.\n\nEthical frameworks based on philosophical traditions are emerging as important considerations. Watanabe (2024) relates AI use in universities to Kant's reflections on enlightenment, suggesting that intelligent tutoring systems and ChatGPT may threaten student intellectual maturity if not properly implemented. Similarly, Alasgarova & Rzayev (2024) apply Self-Determination Theory to examine how AI can enhance learning while potentially undermining students' autonomy and relatedness through misuse.\n\nResearch by Zhou & Schofield (2024) and Usher & Barak (2024) highlights the importance of developing AI ethics literacy for students, especially in STEM fields. Their findings suggest that explicit-reflective learning significantly enhances students' ethical knowledge, awareness, and problem-solving skills related to AI use.\n\nEmpirical studies by Miljkovic Krecar et al. (2024) revealed professors have low accuracy (53%) in distinguishing between student-written and AI-generated papers, suggesting the urgent need for clear guidelines and detection tools. Similarly, Biagini et al. (2024) found doctoral students had little AI knowledge but were overconfident of the technology's capabilities, highlighting the need for comprehensive AI literacy education addressing ethical implications.",
    reflectionPrompts: [
      "How might ethical frameworks for GPT use be adapted to address discipline-specific concerns?",
      "What strategies could help balance the benefits of GPT while preserving essential disciplinary skills?",
      "How can instructors design assessments that maintain academic integrity while allowing appropriate GPT use?"
    ],
    hasWordCloud: true
  },
  {
    id: 3,
    title: "RQ3: Current GPT Practices - Threats and Benefits",
    category: "Implementation",
    summary: "Analysis of current GPT-related practices in education and their positive and negative impacts.",
    content: "Recent literature reveals a range of innovative educational applications of large language models (LLMs) like ChatGPT, alongside legitimate concerns about their implementation. Sonderegger (2022) demonstrated how integrating generative language models with social robots significantly enhances interactive learning capabilities, creating more responsive and engaging educational experiences. Similarly, applications in language education are emerging, with Bonner et al. (2023) documenting practices like automated feedback, personalized learning experiences, and conversational practice for language acquisition.\n\nYu & Guo (2023) identified key challenges in GPT implementation, including opacity of algorithmic processes, data privacy concerns, and questions about effectiveness and personalization. These challenges must be addressed for successful AI integration in education. Grassini (2023) emphasized the need for educators to adapt to rapidly evolving technologies while developing strategies to incorporate AI responsibly.\n\nStudies highlight specialized applications across disciplines. Mageira et al. (2022) found that AI chatbots effectively enhance both content understanding and language proficiency in Content and Language Integrated Learning contexts. In medical education, Abd-alrazaq et al. (2023) identified opportunities for curriculum enhancement, teaching methodologies improvement, and personalized study plans, while cautioning about algorithmic bias, overreliance, and misinformation risks.\n\nFrameworks for effective integration are emerging. Su & Yang (2023) proposed the \"IDEE\" framework (Identifying desired outcomes, Determining automation levels, Ensuring ethical considerations, and Evaluating effectiveness) to guide educators in leveraging AI while addressing challenges. This is complemented by Chang et al. (2023), who advocate for incorporating goal setting, self-assessment feedback, and personalization principles when designing AI chatbots to support self-regulated learning.\n\nPractical applications show promise in specific educational contexts. Akiba & Fraboni (2023) found that ChatGPT delivers high-quality, authoritative responses in academic advising scenarios, particularly excelling in career-related guidance. In science education, Cooper (2023) documented ChatGPT's effectiveness in generating teaching resources and addressing instructional challenges, while noting concerns about its role as a cognitive authority.\n\nInnovative pedagogical approaches are developing in response to GPT tools. Van den Berg & du Plessis (2023) found ChatGPT useful for democratizing access to lesson planning resources but emphasized the need for critical evaluation of AI-generated content. Dai (2024) demonstrated the effectiveness of dual-contrast pedagogy for enhancing AI literacy among elementary students by comparing human and AI characteristics. Ruiz-Rojas et al. (2023) successfully integrated generative AI tools with instructional design matrices to create personalized learning experiences.\n\nLongitudinal research is beginning to emerge. Albadarin et al. (2024) conducted a systematic literature review of empirical ChatGPT studies, identifying applications in personalized learning and administrative efficiency, while highlighting concerns about academic integrity and data privacy. This is supported by Popenici et al. (2023), who found AI has potential to enhance personalized learning, automate administrative tasks, and improve student engagement.\n\nHuman-AI collaboration models are also being explored. Atchley et al. (2024) emphasized developing metacognitive knowledge and skills to enhance learning effectiveness in AI-assisted environments. Kizilcec (2024) highlighted that teacher attitudes, self-efficacy, and trust in AI significantly impact successful integration, emphasizing the importance of understanding educators' perspectives.\n\nImplementation challenges persist as institutions adapt to generative AI. Perkins (2023) documented academic integrity challenges in the post-pandemic era, particularly regarding plagiarism and undetectable AI-generated content. Van Wyk (2024) identified preventive strategies employed by academics, including clear policy guidelines, promoting ethical use, and designing AI-resilient assessments. Giray et al. (2024) found AI enhanced teaching and administrative tasks but raised concerns about academic dishonesty, data fabrication, and potential declines in creativity and critical thinking.\n\nEmerging research examines specific factors affecting implementation success. Gao et al. (2024) surveyed EFL university teachers in China, finding that prior experiences with LLMs and frequency of use significantly influence beliefs about integration, while IT support availability showed less correlation. Estacio Pereira et al. (2024) demonstrated that while LLMs can generate course structures, they often lack contextual awareness for diverse classroom settings, meaning instructor involvement remains essential for customization.",
    reflectionPrompts: [
      "How can instructors and institutions maximize the benefits of GPT while minimizing potential threats?",
      "What frameworks might help distinguish between productive and problematic GPT practices?",
      "How might current GPT practices evolve as both the technology and educational approaches mature?"
    ],
    hasWordCloud: true
  }
];

// Make the data available to the page
window.literatureReviews = literatureReviews; 